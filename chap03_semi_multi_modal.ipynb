{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c333f917",
   "metadata": {},
   "source": [
    "# ***멀티모달 흉내내기***\n",
    "- 하나의 AI에서 영상, 음성, 텍스트를 통합하기 위해서는 아래의 과정이 필요합니다\n",
    "  - 영상의 객체탐지 정보를 언어모델에서 사용하는 숫자(토큰) 정보로 변환\n",
    "  - 음성의 텍스트 정보를 언어모델에서 사용하는 숫자(토큰) 정보로 변환\n",
    "  - 언어모델의 토큰으로 변환된 영상, 음성정보, 우리의 프롬프트를 이용해 다음 문장 생성\n",
    "- 우리가 흉내낼 멀티모달은 아래와 같습니다\n",
    "  - 영상의 객체탐지 정보를 우리의 언어로 변환\n",
    "  - 음성의 텍스트 정보를 우리의 언어로 변환\n",
    "  - 우리의 언어로변환된 영상, 음성정보와 우리의 프롬프트를 언어모델의 토큰으로 변환\n",
    "  - 변환된 토큰으로 다음 문장 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e67ba",
   "metadata": {},
   "source": [
    "## ***영상 객체 탐지 정보를 텍스트로 만들기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8184b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "def detect_objects_with_interval(video_path, interval=1.0):  \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise FileNotFoundError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps\n",
    "\n",
    "    t_secs = np.arange(0, duration, interval)\n",
    "\n",
    "    all_detections = {}\n",
    "    for t_sec in t_secs:\n",
    "        frame_idx = int(t_sec * fps)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            all_detections[t_sec] = []\n",
    "            continue\n",
    "\n",
    "        results = image_model(frame)[0]\n",
    "\n",
    "        detections = []\n",
    "        for box in results.boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "            detections.append({\n",
    "                \"class\": image_model.names[int(box.cls[0])],\n",
    "                \"bbox\": [int(x1), int(y1), int(x2), int(y2)],\n",
    "                \"conf\": float(box.conf[0])\n",
    "            })\n",
    "\n",
    "        all_detections[t_sec] = detections\n",
    "\n",
    "    cap.release()\n",
    "    return all_detections\n",
    "\n",
    "\n",
    "def image_to_text(video_path, interval=1.0):\n",
    "    detections = detect_objects_with_interval(video_path, interval)\n",
    "    lines = []\n",
    "\n",
    "    for t_sec, objs in detections.items():\n",
    "        if not objs:\n",
    "            lines.append(f\"{t_sec:.1f}초에는 탐지된 객체가 없었습니다.\")\n",
    "            continue\n",
    "\n",
    "        descriptions = []\n",
    "        for obj in objs:\n",
    "            cls = obj['class']\n",
    "            x1, y1, x2, y2 = obj['bbox']\n",
    "            descriptions.append(f\"좌표 {x1},{y1}부터 {x2},{y2} 사이에 {cls}이(가) 있었습니다\")\n",
    "\n",
    "        line = f\"{t_sec:.1f}초에는 \" + \" 그리고 \".join(descriptions) + \".\"\n",
    "        lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "video_path = \"movie.mp4\"\n",
    "interval = 1 # 1초 간격으로 영상 샘플링\n",
    "image_text = image_to_text(video_path, interval)\n",
    "print(image_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4e1c8",
   "metadata": {},
   "source": [
    "## ***음성을 타임라인별로 텍스트화 하기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf75f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "voice_model = whisper.load_model(\"small\")\n",
    "\n",
    "def transcribe_audio(video_path):    \n",
    "    result = voice_model.transcribe(video_path)\n",
    "\n",
    "    timeline = []\n",
    "    for seg in result[\"segments\"]:\n",
    "        timeline.append({\n",
    "            \"start\": seg[\"start\"],\n",
    "            \"end\": seg[\"end\"],\n",
    "            \"text\": seg[\"text\"]\n",
    "        })\n",
    "\n",
    "    return timeline\n",
    "\n",
    "def audio_to_text(audio_timeline):\n",
    "    lines = []\n",
    "    for seg in audio_timeline:\n",
    "        lines.append(f\"{seg['start']:.1f}s 부터 {seg['end']:.1f}s 사이에 담긴 음성은 {seg[\"text\"]} 입니다\")\n",
    "        \n",
    "    return \" \".join(lines)\n",
    "\n",
    "\n",
    "audio_timeline = transcribe_audio(video_path)\n",
    "audio_text = audio_to_text(audio_timeline)\n",
    "audio_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa5a3fc",
   "metadata": {},
   "source": [
    "## ***이미지와 음성과 프롬프트를 텍스트로 통합***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01444d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"객체 탐지 정보는 아래와 같습니다\\n{image_text}\\n추출된 음성 정보는 아래와 같습니다\\n{audio_text}\\n주어진 객체와 음성 정보를 기반으로 동영상을 설명해보세요\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7804a630",
   "metadata": {},
   "source": [
    "## ***통합 프롬프트를 언어모델의 입력으로 실행***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ad141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "image_model='llama3.2:1b'\n",
    "\n",
    "messages = [{\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "    }]\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=image_model,\n",
    "    messages=messages,\n",
    "    options={\n",
    "        'temperature': 0.8\n",
    "    })\n",
    "content = response['message']['content']\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659ffbb",
   "metadata": {},
   "source": [
    "## ***동영상으로 시작하여 언어모델에 한번에 연결하기***\n",
    "- 새로운 동영상을 만들고 언어모델까지 한번에 연결해봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "video_path = \"1.mp4\"\n",
    "\n",
    "image_text = image_to_text(video_path, 1)\n",
    "audio_timeline = transcribe_audio(video_path)\n",
    "audio_text = audio_to_text(audio_timeline)\n",
    "prompt = f\"객체 탐지 정보는 아래와 같습니다\\n{image_text}\\n추출된 음성 정보는 아래와 같습니다\\n{audio_text}\\n주어진 객체와 음성 정보를 기반으로 동영상을 설명해보세요\"\n",
    "\n",
    "image_model='llama3.2:1b'\n",
    "\n",
    "messages = [{\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "    }]\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=image_model,\n",
    "    messages=messages,\n",
    "    options={\n",
    "        'temperature': 0.8\n",
    "    })\n",
    "content = response['message']['content']\n",
    "\n",
    "print(content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
